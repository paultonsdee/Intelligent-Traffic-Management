{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class to manage environment variables and settings.\"\"\"\n",
    "\n",
    "    # Email settings\n",
    "    FROM_EMAIL = \"vhp08072004@gmail.com\"\n",
    "    FROM_EMAIL_NICKNAME = \"Traffic Management System (TMS)\"\n",
    "    EMAIL_PASSWORD = \"wayp qcui muhs ietx\"  # Ensure no spaces and is a valid app password\n",
    "    TO_EMAIL = \"vhp08071974@gmail.com\"\n",
    "\n",
    "    # Gemini API\n",
    "    GEMINI_API_KEY = \"AIzaSyBf1wGteqDPnWEjRaCM5JvVRptFe5UMUQg\"\n",
    "\n",
    "    # Model paths\n",
    "    CNN_MODEL_PATH = \"CNN_model.onnx\"\n",
    "    LSTM_MODEL_PATH = \"LSTM_model.onnx\"\n",
    "    ULTRALYTICS_MODEL_PATH = \"yolo11n.pt\"\n",
    "\n",
    "    # Speed Estimator Model Path\n",
    "    SPEED_ESTIMATOR_MODEL_PATH = \"yolo11n.pt\"\n",
    "\n",
    "    # Violation Thresholds\n",
    "    VIOLATION_THRESHOLDS = {\n",
    "        'large_vehicles': 10,\n",
    "        'motorcycles': 15,\n",
    "        'illegal_parking': 5,\n",
    "        'red_light': 7\n",
    "    }\n",
    "\n",
    "    # Logging settings\n",
    "    LOG_FILE = \"system.log\"\n",
    "    LOG_MAX_BYTES = 5 * 1024 * 1024  # 5 MB\n",
    "    LOG_BACKUP_COUNT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phatvu/miniconda3/envs/aiot/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_gemini():\n",
    "    \"\"\"\n",
    "    Configures the Gemini API using the provided API key.\n",
    "    Returns the configured Gemini model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        genai.configure(api_key=Config.GEMINI_API_KEY)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.5-flash-8b\")\n",
    "        print(\"Gemini API configured successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring Gemini API: {e}\")\n",
    "        return None\n",
    "\n",
    "model_gemini = configure_gemini()\n",
    "\n",
    "def parse_gemini_response(response_text):\n",
    "    \"\"\"\n",
    "    Parses the Gemini API response to extract ambulance bounding box data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract JSON code block using regex\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if not json_match:\n",
    "            return None\n",
    "        json_str = json_match.group(0)\n",
    "        data = json.loads(json_str)\n",
    "        if \"ambulance\" in data:\n",
    "            return data[\"ambulance\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing Gemini response: {e}\")\n",
    "    return None\n",
    "\n",
    "# --------------------- PyTorch Configuration --------------------- #\n",
    "\n",
    "def configure_pytorch():\n",
    "    \"\"\"\n",
    "    Configures PyTorch to use MPS, CUDA, or CPU.\n",
    "    Returns the device being used.\n",
    "    \"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(\"Using device: MPS\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        cuda_name = torch.cuda.get_device_name(device)\n",
    "        print(f\"Using device: {cuda_name}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using device: CPU\")\n",
    "    return device\n",
    "\n",
    "device_pt = configure_pytorch()\n",
    "\n",
    "# --------------------- Feature Extraction and Prediction Functions --------------------- #\n",
    "\n",
    "def features_extractor(audio, sample_rate):\n",
    "    \"\"\"\n",
    "    Extracts MFCC features from audio data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=80)\n",
    "        mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "        return mfccs_scaled_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_onnx_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads an ONNX model and returns an ONNX Runtime session.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = ort.InferenceSession(model_path)\n",
    "        print(f\"Loaded ONNX model from {model_path}\")\n",
    "        return session\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ONNX model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load models\n",
    "cnn_session = load_onnx_model(Config.CNN_MODEL_PATH)\n",
    "lstm_session = load_onnx_model(Config.LSTM_MODEL_PATH)\n",
    "\n",
    "def predict_cnn(features, session):\n",
    "    \"\"\"\n",
    "    Predicts the class using a CNN ONNX model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare input\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        input_data = features.astype(np.float32).reshape(1, -1, 1)\n",
    "        predictions = session.run(None, {input_name: input_data})\n",
    "        predicted_class = np.argmax(predictions[0], axis=1)\n",
    "        return predicted_class[0], predictions[0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"CNN prediction failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def predict_lstm(features, session):\n",
    "    \"\"\"\n",
    "    Predicts the class using an LSTM ONNX model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare input\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        input_data = features.astype(np.float32).reshape(1, -1, 80)\n",
    "        predictions = session.run(None, {input_name: input_data})\n",
    "        predicted_class = np.argmax(predictions[0], axis=1)\n",
    "        return predicted_class[0], predictions[0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"LSTM prediction failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def ensemble_predictions(cnn_probs, lstm_probs):\n",
    "    \"\"\"\n",
    "    Averages the probabilities from CNN and LSTM models for ensemble prediction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ensemble_probs = (cnn_probs + lstm_probs) / 2  # Average probabilities\n",
    "        return ensemble_probs\n",
    "    except Exception as e:\n",
    "        print(f\"Ensemble prediction failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiDetectionThread(QThread):\n",
    "    \"\"\"\n",
    "    Thread to handle ambulance detection using the Gemini API.\n",
    "    Emits a signal indicating whether bounding boxes were detected.\n",
    "    \"\"\"\n",
    "    bounding_boxes_detected = pyqtSignal(bool)  # Emits True if bounding boxes are detected\n",
    "\n",
    "    def __init__(self, video_file, region3, start_frame, fps, parent=None):\n",
    "        super().__init__(parent)\n",
    "        self.video_file = video_file\n",
    "        self.region3 = region3\n",
    "        self.start_frame = start_frame\n",
    "        self.fps = fps\n",
    "        self._run_flag = True\n",
    "        self.model = model_gemini\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def run(self):\n",
    "        if not self.model:\n",
    "            self.logger.error(\"Gemini model not initialized.\")\n",
    "            self.bounding_boxes_detected.emit(False)\n",
    "            return\n",
    "\n",
    "        cap = cv2.VideoCapture(self.video_file)\n",
    "        if not cap.isOpened():\n",
    "            self.logger.error(\"Failed to open video file.\")\n",
    "            self.bounding_boxes_detected.emit(False)\n",
    "            return\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_interval = int(self.fps * 3 / 5)\n",
    "        frames_to_extract = [self.start_frame + i * frame_interval for i in range(5)]\n",
    "        responses = []\n",
    "\n",
    "        for frame_num in frames_to_extract:\n",
    "            if frame_num >= total_frames:\n",
    "                break\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            roi = self.crop_to_region(frame, self.region3)\n",
    "            has_bbox = self.send_to_gemini(roi)\n",
    "            responses.append(has_bbox)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        bbox_count = sum(responses)\n",
    "        no_bbox_count = len(responses) - bbox_count\n",
    "        if bbox_count > no_bbox_count:\n",
    "            # Switch traffic light to green\n",
    "            self.bounding_boxes_detected.emit(True)\n",
    "            self.logger.info(\"Bounding boxes detected more than non-detections.\")\n",
    "            # Continue detection every 2 seconds\n",
    "            while self._run_flag:\n",
    "                time.sleep(2)\n",
    "                next_frame = min(self.start_frame + int(self.fps * 3), total_frames - 1)\n",
    "                cap = cv2.VideoCapture(self.video_file)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, next_frame)\n",
    "                ret, frame = cap.read()\n",
    "                cap.release()\n",
    "                if not ret:\n",
    "                    break\n",
    "                roi = self.crop_to_region(frame, self.region3)\n",
    "                has_bbox = self.send_to_gemini(roi)\n",
    "                if has_bbox:\n",
    "                    continue  # Continue looping\n",
    "                else:\n",
    "                    # No bounding box detected, reset traffic light\n",
    "                    self.bounding_boxes_detected.emit(False)\n",
    "                    self.logger.info(\"No bounding boxes detected in subsequent frames.\")\n",
    "                    break\n",
    "        else:\n",
    "            # Not enough bounding boxes detected\n",
    "            self.bounding_boxes_detected.emit(False)\n",
    "            self.logger.info(\"Bounding boxes not detected sufficiently.\")\n",
    "\n",
    "    def crop_to_region(self, frame, region):\n",
    "        pts = np.array(region, np.int32)\n",
    "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "        cv2.fillPoly(mask, [pts], 255)\n",
    "        roi = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "        x, y, w, h = cv2.boundingRect(pts)\n",
    "        roi_cropped = roi[y:y+h, x:x+w]\n",
    "        return roi_cropped\n",
    "\n",
    "    def send_to_gemini(self, image):\n",
    "        \"\"\"\n",
    "        Sends the cropped region to the Gemini API for ambulance detection.\n",
    "        \"\"\"\n",
    "        if image.size == 0:\n",
    "            self.logger.warning(\"Empty ROI image, skipping Gemini detection.\")\n",
    "            return False\n",
    "\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        try:\n",
    "            response = self.model.generate_content(\n",
    "                [PROMPT, pil_image],\n",
    "            ).text\n",
    "            bbox = parse_gemini_response(response)\n",
    "            if bbox:\n",
    "                self.logger.info(\"Bounding box detected by Gemini.\")\n",
    "                return True\n",
    "            else:\n",
    "                self.logger.info(\"No bounding box detected by Gemini.\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Gemini API call failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stops the thread gracefully.\n",
    "        \"\"\"\n",
    "        self._run_flag = False\n",
    "        self.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
